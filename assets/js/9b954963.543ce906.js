"use strict";(self.webpackChunkwaxdevelopers=self.webpackChunkwaxdevelopers||[]).push([[5382],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>h});var n=a(7294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,r=e.originalType,l=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),c=p(a),m=s,h=c["".concat(l,".").concat(m)]||c[m]||u[m]||r;return a?n.createElement(h,o(o({ref:t},d),{},{components:a})):n.createElement(h,o({ref:t},d))}));function h(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var r=a.length,o=new Array(r);o[0]=m;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i[c]="string"==typeof e?e:s,o[1]=i;for(var p=2;p<r;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},2364:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>p});var n=a(7462),s=(a(7294),a(3905));const r={title:"Atomic Assets/Market API",nav_order:143,layout:"default",parent:"WAX Infrastructure/APIs","lang-ref":"Atomic Assets/Market API",lang:"en"},o=void 0,i={unversionedId:"wax-infrastructure/atomic-api-guide",id:"wax-infrastructure/atomic-api-guide",title:"Atomic Assets/Market API",description:"WAX Blockchain is hevaily focused on NFTs and AtomicAssets standard has became the defacto standard on WAX for NFTs. This guide helps project teams to setup their own AtomicAssets API Infrastructure to access NFTs and Marketplaces specific data. This API data can be useful for various purposes like:",source:"@site/docs/wax-infrastructure/atomic-api-guide.md",sourceDirName:"wax-infrastructure",slug:"/wax-infrastructure/atomic-api-guide",permalink:"/waxdevelopers_test/docs/wax-infrastructure/atomic-api-guide",draft:!1,editUrl:"https://github.com/3dkrender/waxdevelopers_test/tree/main/docs/wax-infrastructure/atomic-api-guide.md",tags:[],version:"current",frontMatter:{title:"Atomic Assets/Market API",nav_order:143,layout:"default",parent:"WAX Infrastructure/APIs","lang-ref":"Atomic Assets/Market API",lang:"en"},sidebar:"tutorialSidebar",previous:{title:"API Full/Partial Archive nodes",permalink:"/waxdevelopers_test/docs/wax-infrastructure/api-archive-guide"},next:{title:"Full/Partial History nodes using Hyperion",permalink:"/waxdevelopers_test/docs/wax-infrastructure/hyperion-guide"}},l={},p=[{value:"Pre-requisites/Requirements:",id:"pre-requisitesrequirements",level:3},{value:"Bare-Metal Infra providers:",id:"bare-metal-infra-providers",level:4},{value:"Cloud Infra providers:",id:"cloud-infra-providers",level:4},{value:"Setup and Installation:",id:"setup-and-installation",level:3},{value:"1. Update the default pacakages and install new ones",id:"1-update-the-default-pacakages-and-install-new-ones",level:5},{value:"2. For better CPU performance:",id:"2-for-better-cpu-performance",level:5},{value:"3. Create disk partitions",id:"3-create-disk-partitions",level:5},{value:"4. Increase the Swap size as its usually small on the servers from Hetzner and Leaseweb.",id:"4-increase-the-swap-size-as-its-usually-small-on-the-servers-from-hetzner-and-leaseweb",level:5},{value:"5. Create ZFS storage pool based on your requirements with zraid or mirror etc modes. A good resource to do calculations on disk sizes: http://www.raidz-calculator.com/",id:"5-create-zfs-storage-pool-based-on-your-requirements-with-zraid-or-mirror-etc-modes-a-good-resource-to-do-calculations-on-disk-sizes-httpwwwraidz-calculatorcom",level:5},{value:"6. Node JS installation:",id:"6-node-js-installation",level:5},{value:"7. PostgreSQL 14 Setup &amp; Installation:",id:"7-postgresql-14-setup--installation",level:5},{value:"Replace the following sections in the PG config file",id:"replace-the-following-sections-in-the-pg-config-file",level:6},{value:"8. Redis installation",id:"8-redis-installation",level:5},{value:"Update Redis Supervision Method",id:"update-redis-supervision-method",level:6},{value:"Restart Redis",id:"restart-redis",level:6},{value:"9. PGAdmin4 installation",id:"9-pgadmin4-installation",level:5},{value:"10. Yarn and Pm2 installation",id:"10-yarn-and-pm2-installation",level:5},{value:"11. Setup &amp; Install Atomic Filler &amp; API:",id:"11-setup--install-atomic-filler--api",level:5},{value:"Setup:",id:"setup",level:6},{value:"connections.config.json",id:"connectionsconfigjson",level:4},{value:"readers.config.json",id:"readersconfigjson",level:4},{value:"server.config.json",id:"serverconfigjson",level:4},{value:"Running Hyperion:",id:"running-hyperion",level:6},{value:"Docker",id:"docker",level:3},{value:"PM2",id:"pm2",level:3},{value:"Good to Know: Currently Supported Contracts",id:"good-to-know-currently-supported-contracts",level:2},{value:"Readers (used to fill the database)",id:"readers-used-to-fill-the-database",level:3},{value:"atomicassets",id:"atomicassets",level:4},{value:"atomicmarket",id:"atomicmarket",level:4},{value:"delphioracle",id:"delphioracle",level:4},{value:"Namespace (API endpoints)",id:"namespace-api-endpoints",level:3},{value:"atomicassets",id:"atomicassets-1",level:4},{value:"atomicmarket",id:"atomicmarket-1",level:4}],d={toc:p},c="wrapper";function u(e){let{components:t,...a}=e;return(0,s.kt)(c,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("p",null,"WAX Blockchain is hevaily focused on NFTs and AtomicAssets standard has became the defacto standard on WAX for NFTs. This guide helps project teams to setup their own AtomicAssets API Infrastructure to access NFTs and Marketplaces specific data. This API data can be useful for various purposes like:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Building Games"),(0,s.kt)("li",{parentName:"ul"},"Building Marketplaces"),(0,s.kt)("li",{parentName:"ul"},"Accessing NFTs in games"),(0,s.kt)("li",{parentName:"ul"},"Users NFTs Portfolio"),(0,s.kt)("li",{parentName:"ul"},"Users NFTs Historical activities"),(0,s.kt)("li",{parentName:"ul"},"For Accounting and Tax purposes")),(0,s.kt)("h3",{id:"pre-requisitesrequirements"},"Pre-requisites/Requirements:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Atomic Filler node Hardware(minimum specs):")," Multi-threaded CPU with at-least 4gHZ CPU speed or above, 64GB RAM, 2TB NvME SSD"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Atomic API Hardware(minimum specs):")," Multi-threaded CPU with at-least 4gHZ CPU speed or above, 128GB RAM, 2TB NvME SSD"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Full State-History node Hardware(recommended specs):")," i9 CPU, 128GB RAM, 7TB NVME SSD ","[For a partial state-history, you can have lower specs or have it on the same server as Atomic Filler. This can also be started from a snapshot]"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Requirements:")," ",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},"PostgreSQL >= 13.0"),(0,s.kt)("li",{parentName:"ul"},"NodeJS >= 16.0"),(0,s.kt)("li",{parentName:"ul"},"Redis >= 5.0"),(0,s.kt)("li",{parentName:"ul"},"Nodeos >= 1.8.0 (only tested with 2.0 and 2.1) The state history plugin needs to be enabled and the options: ",(0,s.kt)("inlineCode",{parentName:"li"},"trace-history = true"),", ",(0,s.kt)("inlineCode",{parentName:"li"},"chain-state-history = true")),(0,s.kt)("li",{parentName:"ul"},"Hasura GraphQL Engine >= 1.3 (if you want to allow GraphQL queries) ","[https://computingforgeeks.com/install-hasura-graphql-engine-on-ubuntu-18-04-centos-7/]"),(0,s.kt)("li",{parentName:"ul"},"PGAdmin 4 (Interface to manage the postgres database)")))),(0,s.kt)("p",null,"You can use 1 node for both Atomic filler and API in a 128GB RAM server but it's recommended to have a High Availability setup using Postgres replication between the servers for better performance and request handling. "),(0,s.kt)("h4",{id:"bare-metal-infra-providers"},"Bare-Metal Infra providers:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://www.hetzner.com/dedicated-rootserver",title:"Hetzner"},"Hetzner")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://www.leaseweb.us/dedicated-servers",title:"Leaseweb"},"Leaseweb"))),(0,s.kt)("h4",{id:"cloud-infra-providers"},"Cloud Infra providers:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://www.digitalocean.com/pricing/managed-databases"},"https://www.digitalocean.com/pricing/managed-databases"))),(0,s.kt)("h3",{id:"setup-and-installation"},"Setup and Installation:"),(0,s.kt)("p",null,"After securing the servers or cloud instances and setting up the boot configuration and appropriate RAID modes, you can login to the server and follow the next commands below:"),(0,s.kt)("p",null,"[Recommendation - Only setup root partition in Raid1 or Raid5 modes for now. We shall partition the disks later on after the boot and allocate them to a ZFS pool]"),(0,s.kt)("h5",{id:"1-update-the-default-pacakages-and-install-new-ones"},"1. Update the default pacakages and install new ones"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"apt-get update && apt-get install -y vim htop aptitude git lxc-utils zfsutils-linux netfilter-persistent sysstat ntp gpg screen zstd\n")),(0,s.kt)("h5",{id:"2-for-better-cpu-performance"},"2. For better CPU performance:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"apt-get install -y cpufrequtils\necho 'GOVERNOR=\"performance\"' | tee /etc/default/cpufrequtils\nsystemctl disable ondemand\nsystemctl restart cpufrequtils\n")),(0,s.kt)("h5",{id:"3-create-disk-partitions"},"3. Create disk partitions"),(0,s.kt)("p",null,"First step is to determine the disks and their names using the commands below:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"fdisk -l\n")),(0,s.kt)("p",null,"Now after identifying the disk names, let's partition them using the example command below, we need to create two partitions One for Swap and One for ZFS storage pool."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"cfdisk /dev/nvme0n1\n")),(0,s.kt)("p",null,"Do the above for all the disks on your server."),(0,s.kt)("h5",{id:"4-increase-the-swap-size-as-its-usually-small-on-the-servers-from-hetzner-and-leaseweb"},"4. Increase the Swap size as its usually small on the servers from Hetzner and Leaseweb."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"mkswap /dev/nvme0n1p5\nmkswap /dev/nvme1n1p5\n")),(0,s.kt)("p",null,"Now let's add the Swap pools to the System's FileSystem table by editing the file below:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"cat >>/etc/fstab <<'EOT'\n/dev/nvme0n1p5     none            swap            defaults,pri=-2 0 0\n/dev/nvme1n1p5     none            swap            defaults,pri=-2 0 0\nEOT\n")),(0,s.kt)("p",null,"After editing, let's enable the newly added Swap pool using the command below:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"swapon -a\n")),(0,s.kt)("h5",{id:"5-create-zfs-storage-pool-based-on-your-requirements-with-zraid-or-mirror-etc-modes-a-good-resource-to-do-calculations-on-disk-sizes-httpwwwraidz-calculatorcom"},"5. Create ZFS storage pool based on your requirements with zraid or mirror etc modes. A good resource to do calculations on disk sizes: ",(0,s.kt)("a",{parentName:"h5",href:"http://www.raidz-calculator.com/"},"http://www.raidz-calculator.com/")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"zpool create -o ashift=12 zfast raidz /dev/nvme0n1p6 /dev/nvme1n1p6 [--\x3e adopt the partition names accordingly]\nzfs set atime=off zfast\nzfs set compression=lz4 zfast \nzfs create -o mountpoint=/home zfast/home [--\x3eCreates mountpoint]\n")),(0,s.kt)("hr",null),(0,s.kt)("p",null,"Now that we have setup the server and disk storage in a good way, let's go ahead with the next steps to setup the Hyperion related dependencies."),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://hyperion.docs.eosrio.io/manual_installation/"},"https://hyperion.docs.eosrio.io/manual_installation/")),(0,s.kt)("h5",{id:"6-node-js-installation"},"6. Node JS installation:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -\nsudo apt-get install -y nodejs\nnode  -v\n")),(0,s.kt)("h5",{id:"7-postgresql-14-setup--installation"},"7. PostgreSQL 14 Setup & Installation:"),(0,s.kt)("p",null,"The following steps are for a single node setup but it is recommended to have a multi-node postgres cluster with repliacation for better performance & resiliency. "),(0,s.kt)("p",null,"For Postgres replication cluster setup, refer:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://girders.org/postgresql/2021/11/05/setup-postgresql14-replication/"},"Streaming Replication"))),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"PostgreSQL 14 Installation using Apt package:")),(0,s.kt)("p",null,"Guide: ",(0,s.kt)("a",{parentName:"p",href:"https://techviewleo.com/how-to-install-postgresql-database-on-ubuntu/"},"https://techviewleo.com/how-to-install-postgresql-database-on-ubuntu/")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"sudo apt update && sudo apt -y upgrade\nsudo apt -y install gnupg2 wget vim\nsudo apt-cache search postgresql | grep postgresql\nsudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt -y update\nsudo apt -y install postgresql-14 postgresql-client-14\n")),(0,s.kt)("p",null,"On successful installation, the PostgreSQL service starts automatically and can be verified as below."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"$ systemctl status postgresql\n\u25cf postgresql.service - PostgreSQL RDBMS\n   Loaded: loaded (/lib/systemd/system/postgresql.service; enabled; vendor preset: enabled)\n   Active: active (exited) since Mon 2021-10-25 16:15:55 CEST; 5s ago\n  Process: 32506 ExecStart=/bin/true (code=exited, status=0/SUCCESS)\n Main PID: 32506 (code=exited, status=0/SUCCESS)\n\nOkt 25 16:15:55 thor-KVM systemd[1]: Starting PostgreSQL RDBMS...\nOkt 25 16:15:55 thor-KVM systemd[1]: Started PostgreSQL RDBMS.\n")),(0,s.kt)("p",null,"You can also verify the installed PostgreSQL version using the command below:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'sudo -u postgres psql -c "SELECT version();"\n')),(0,s.kt)("p",null,"After verifying installation, Let's update the password for user postgres:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"sudo su - postgres\npsql -c \"alter user postgres with password '<ENTER YOUR PASSWORD HERE>'\n")),(0,s.kt)("p",null,"Now, let's create new directories on the ZFS storage pool so that PG data can be stored there instead of default directories:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"cd /home\nmkdir pg-data\nchown -R postgres:postgres pg-data/\n")),(0,s.kt)("p",null,"After creating the directories and fixing the folder permissions, let's edit the PG config by editing the file below:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"vim /etc/postgresql/14/main/postgresql.conf\n")),(0,s.kt)("h6",{id:"replace-the-following-sections-in-the-pg-config-file"},"Replace the following sections in the PG config file"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"data_directory = '/var/lib/postgresql/14/main'\n")),(0,s.kt)("h5",{id:"8-redis-installation"},"8. Redis installation"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"sudo add-apt-repository ppa:redislabs/redis\nsudo apt-get -y update\nsudo apt-get -y install redis\nredis-server -v\n")),(0,s.kt)("h6",{id:"update-redis-supervision-method"},"Update Redis Supervision Method"),(0,s.kt)("p",null,"Change the ",(0,s.kt)("inlineCode",{parentName:"p"},"supervised")," configuration from ",(0,s.kt)("inlineCode",{parentName:"p"},"supervised no")," to ",(0,s.kt)("inlineCode",{parentName:"p"},"supervised systemd")," on ",(0,s.kt)("inlineCode",{parentName:"p"},"/etc/redis/redis.conf")),(0,s.kt)("h6",{id:"restart-redis"},"Restart Redis"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"sudo systemctl restart redis-server\nsudo systemctl enable --now redis-server\nsudo systemctl status redis-server\nsudo systemctl status redis-server\nsudo systemctl unmask  redis-server.service\nsudo systemctl restart redis-server\nsudo systemctl status redis-server\n")),(0,s.kt)("h5",{id:"9-pgadmin4-installation"},"9. PGAdmin4 installation"),(0,s.kt)("p",null,"Guides:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://computingforgeeks.com/how-to-install-pgadmin-4-on-ubuntu/"},"https://computingforgeeks.com/how-to-install-pgadmin-4-on-ubuntu/")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://stackoverflow.com/questions/58239607/pgadmin-package-pgadmin4-has-no-installation-candidate"},"https://stackoverflow.com/questions/58239607/pgadmin-package-pgadmin4-has-no-installation-candidate"))),(0,s.kt)("p",null,"Installation commands:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"curl https://www.pgadmin.org/static/packages_pgadmin_org.pub | sudo apt-key add\n\nsudo sh -c 'echo \"deb https://ftp.postgresql.org/pub/pgadmin/pgadmin4/apt/$(lsb_release -cs) pgadmin4 main\" > /etc/apt/sources.list.d/pgadmin4.list && apt update'\n\nsudo apt -y install pgadmin4\n\nFor setting up the web server: sudo /usr/pgadmin4/bin/setup-web.sh\n")),(0,s.kt)("h5",{id:"10-yarn-and-pm2-installation"},"10. Yarn and Pm2 installation"),(0,s.kt)("p",null,"Yarn Installation:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'sudo apt remove -y cmdtest\nsudo apt remove -y yarn\ncurl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -\necho "deb https://dl.yarnpkg.com/debian/ stable main" | sudo tee /etc/apt/sources.list.d/yarn.list\nsudo apt-get update -y\nsudo apt-get install yarn -y\n')),(0,s.kt)("p",null,"Pm2 instllation:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"yarn global add pm2\npm2 startup\n")),(0,s.kt)("hr",null),(0,s.kt)("h5",{id:"11-setup--install-atomic-filler--api"},"11. Setup & Install Atomic Filler & API:"),(0,s.kt)("p",null,"Now we have finished the dependencies setup, let's go ahead and start the actual Atomic API software installation."),(0,s.kt)("p",null,"We have two options now:"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"To install and sync everything from scratch. "),(0,s.kt)("li",{parentName:"ol"},"Use existing PG snapshots to sync the data and then start the Atomic Filler & API instances.")),(0,s.kt)("p",null,"Note: If you are using PG snapshots from a snapshot service provider, download and extract the snapshots"),(0,s.kt)("h6",{id:"setup"},"Setup:"),(0,s.kt)("p",null,"Clone the latest codebase and install the Atomic Filler & API:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"git clone https://github.com/pinknetworkx/eosio-contract-api.git\ncd eosio-contract-api\nyarn install\n")),(0,s.kt)("p",null,"Now it's installed, we have to setup the connections and the chain configuration."),(0,s.kt)("p",null,"Follow the guide ",(0,s.kt)("a",{parentName:"p",href:"https://github.com/pinknetworkx/eosio-contract-api/blob/master/README.md",title:"here"},"here")," to setup configuration files. or find the examples below:"),(0,s.kt)("p",null,"The config folder contains 3 different configuration files"),(0,s.kt)("h4",{id:"connectionsconfigjson"},"connections.config.json"),(0,s.kt)("p",null,"This file contains Postgres / Redis / Nodeos connection data for the used chain."),(0,s.kt)("p",null,"Notes"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Redis: Can be used for multiple chains without further action"),(0,s.kt)("li",{parentName:"ul"},"PostgreSQL: Each chain needs it own postgres database (can use the same postgres instance), but multiple readers of the same\nchain can use the same database if they are non conflicting"),(0,s.kt)("li",{parentName:"ul"},"Nodeos: nodeos should habe a full state history for the range you are trying to index")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "postgres": {\n    "host": "127.0.0.1",\n    "port": 5432,\n    "user": "username",\n    "password": "changeme",\n    "database": "api-wax-mainnet-atomic-1"\n  },\n  "redis": {\n    "host": "127.0.0.1",\n    "port": 6379\n  },\n  "chain": {\n    "name": "wax-mainnet",\n    "chain_id": "1064487b3cd1a897ce03ae5b6a865651747e2e152090f99c1d19d44e01aea5a4",\n    "http": "http://127.0.0.1:8888",\n    "ship": "ws://127.0.0.1:8080"\n  }\n}\n')),(0,s.kt)("h4",{id:"readersconfigjson"},"readers.config.json"),(0,s.kt)("p",null,"This file is used to configure the filler"),(0,s.kt)("p",null,"For atomicassets / atomicmarket you should specify the following start blocks"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"wax-mainnet"),": ",(0,s.kt)("inlineCode",{parentName:"li"},"64000000")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"wax-testnet"),": ",(0,s.kt)("inlineCode",{parentName:"li"},"35795000")," (Here you need to use it otherwise it will break)")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json5"},'[\n  // Multiple Readers can be defined and each one will run in a separated thread\n  {\n    "name": "atomic-1", // Name of the reader. Should be unique per chain and should not change after it was started\n\n    "start_block": 0, // start at a specific block. If ready was already started, this can only be higher than the last indexed block\n    "stop_block": 0, // stop at a specific block\n    "irreversible_only": false, // If you need data for a lot of contracts and do not need live data, this option is faster\n\n    "ship_prefetch_blocks": 50, // How much unconfirmed blocks ship will send\n    "ship_min_block_confirmation": 30, // After how much blocks the reader will confirm the blocks\n    "ship_ds_queue_size": 20, // how much blocks the reader should preserialize the action / table data\n      \n    "ds_ship_threads": 4, // How much threads should be used to deserialize traces and table deltas\n\n    "db_group_blocks": 10, // In catchup mode, the reader will group this amount of blocks\n\n    "contracts": [\n      // AtomicAssets handler which provides data for the AtomicAssets NFT standard\n      {\n        "handler": "atomicassets",\n        "args": {\n          "atomicassets_account": "atomicassets", // Account where the contract is deployed\n          "store_logs": true, // store logs\n          "store_transfers": true // store the transfer history\n        }\n      }\n    ]\n  }\n]\n')),(0,s.kt)("h4",{id:"serverconfigjson"},"server.config.json"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json5"},'{\n  "provider_name": "pink.network", // Provider which is show in the endpoint documentation\n  "provider_url": "https://pink.network",\n\n  "server_addr": "0.0.0.0", // Server address to bind to\n  "server_name": "wax.api.atomicassets.io", // Server name which is shown in the documentation\n  "server_port": 9000, // Server Port\n\n  "cache_life": 2, // GET endpoints are cached for this amount of time (in seconds)\n  "trust_proxy": true, // Enable if you use a reverse proxy to have correct rate limiting by ip\n\n  "rate_limit": {\n    "interval": 60, // Interval to reset the counter (in seconds)\n    "requests": 240 // How much requests can be made in the defined interval\n  },\n    \n  "ip_whitelist": [], // These IPs are not rate limited or receive cached requests\n  "slow_query_threshold": 7500, // If specific queries take longer than this threshold a warning is created\n\n  "max_query_time_ms": 10000, // max execution time for a database query\n  "max_db_connections": 50, // max number of concurrent db connections / db queries\n        \n  "namespaces": [\n    // atomicassets namespace which provides an API for basic functionalities\n    {\n      "name": "atomicassets", \n      "path": "/atomicassets", // Each API endpoint will start with this path\n      "args": {\n        "atomicassets_account": "atomicassets" // Account where the contract is deployed\n      }\n    }\n  ]\n}\n\n')),(0,s.kt)("h6",{id:"running-hyperion"},"Running Hyperion:"),(0,s.kt)("p",null,"This project consists of two separated processes which need to be started and stopped independently:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"The API which will provide the socket and REST endpoints (or whatever is used)"),(0,s.kt)("li",{parentName:"ul"},"The filler which will read the data from the blockchain and fills the database")),(0,s.kt)("p",null,"The filler needs to be started before the API when running it for the first time:"),(0,s.kt)("p",null,"Prerequisites:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},"PostgreSQL"),(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},"Create a database and user which is allowed to read and write on that db"))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},"WAX Archive node "),(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},"State History Plugin enabled with options ",(0,s.kt)("inlineCode",{parentName:"li"},"trace-history = true"),", ",(0,s.kt)("inlineCode",{parentName:"li"},"chain-state-history = true")),(0,s.kt)("li",{parentName:"ul"},"Fully synced for the block range you want to process"),(0,s.kt)("li",{parentName:"ul"},"Open socket and http api"))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("p",{parentName:"li"},"Copy and modify example configs with the correct connection params"))),(0,s.kt)("p",null,"There are two suggested ways to run the project: Docker if you want to containerize the application or PM2 if you want to run it on system level"),(0,s.kt)("h3",{id:"docker"},"Docker"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("inlineCode",{parentName:"li"},"git clone && cd eosio-contract-api")),(0,s.kt)("li",{parentName:"ol"},"There is an example docker compose file provided"),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("inlineCode",{parentName:"li"},"docker-compose up -d"))),(0,s.kt)("p",null,"Start"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"docker-compose start eosio-contract-api-filler")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"docker-compose start eosio-contract-api-server"))),(0,s.kt)("p",null,"Stop"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"docker-compose stop eosio-contract-api-filler")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"docker-compose stop eosio-contract-api-server"))),(0,s.kt)("h3",{id:"pm2"},"PM2"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("inlineCode",{parentName:"li"},"git clone && cd eosio-contract-api")),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("inlineCode",{parentName:"li"},"yarn install")),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("inlineCode",{parentName:"li"},"yarn global add pm2"))),(0,s.kt)("p",null,"Start"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"pm2 start ecosystems.config.json --only eosio-contract-api-filler")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"pm2 start ecosystems.config.json --only eosio-contract-api-server"))),(0,s.kt)("p",null,"Stop"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"pm2 stop eosio-contract-api-filler")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("inlineCode",{parentName:"li"},"pm2 stop eosio-contract-api-server"))),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Note:")," If you have any further questions, please write them here: ",(0,s.kt)("a",{parentName:"p",href:"https://t.me/waxinfra"},"https://t.me/waxinfra")),(0,s.kt)("hr",null),(0,s.kt)("h2",{id:"good-to-know-currently-supported-contracts"},"Good to Know: Currently Supported Contracts"),(0,s.kt)("h3",{id:"readers-used-to-fill-the-database"},"Readers (used to fill the database)"),(0,s.kt)("p",null,"Readers are used to fill the database for a specific contract."),(0,s.kt)("h4",{id:"atomicassets"},"atomicassets"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json5"},'{\n  "handler": "atomicassets",\n  "args": {\n    "atomicassets_account": "atomicassets", // account where the atomicassets contract is deployed\n    "store_transfers": true, // store the transfer history  \n    "store_logs": true // store data structure logs\n  }\n}\n')),(0,s.kt)("h4",{id:"atomicmarket"},"atomicmarket"),(0,s.kt)("p",null,"This reader requires a atomicassets and a delphioracle reader with the same contract as specified here"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json5"},'{\n  "handler": "atomicmarket",\n  "args": {\n    "atomicassets_account": "atomicassets", // account where the atomicassets contract is deployed\n    "atomicmarket_account": "atomicmarket", // account where the atomicmarket contract is deployed\n    "store_logs": true // Store logs of sales / auctions\n  }\n}\n')),(0,s.kt)("h4",{id:"delphioracle"},"delphioracle"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json5"},'{\n  "handler": "delphioracle",\n  "args": {\n    "delphioracle_account": "delphioracle" // account where the delphioracle contract is deployed\n  }\n}\n')),(0,s.kt)("h3",{id:"namespace-api-endpoints"},"Namespace (API endpoints)"),(0,s.kt)("p",null,"A namespace provides an API for a specific contract or use case and is based on data a reader provides"),(0,s.kt)("h4",{id:"atomicassets-1"},"atomicassets"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json5"},'{\n  "handler": "atomicassets",\n  "args": {\n    "atomicassets_account": "atomicassets", // account where the atomicassets contract is deployed\n    "connected_reader": "atomic-1" // reader to which the API connects for live data\n  }\n}\n')),(0,s.kt)("h4",{id:"atomicmarket-1"},"atomicmarket"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-json5"},'{\n  "handler": "atomicmarket",\n  "args": {\n    "atomicmarket_account": "atomicmarket", // account where the atomicmarket contract is deployed\n    "connected_reader": "atomic-1" // reader to which the API connects for live data\n  }\n}\n')))}u.isMDXComponent=!0}}]);