"use strict";(self.webpackChunkwaxdevelopers=self.webpackChunkwaxdevelopers||[]).push([[450],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>m});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},s=Object.keys(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,s=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),c=p(a),h=r,m=c["".concat(l,".").concat(h)]||c[h]||u[h]||s;return a?n.createElement(m,i(i({ref:t},d),{},{components:a})):n.createElement(m,i({ref:t},d))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=a.length,i=new Array(s);i[0]=h;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[c]="string"==typeof e?e:r,i[1]=o;for(var p=2;p<s;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},3213:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const s={title:"Full/Partial History nodes using Hyperion",nav_order:142,layout:"default",parent:"WAX Infrastructure Guides","lang-ref":"Full/Partial History nodes using Hyperion",lang:"en"},i=void 0,o={unversionedId:"wax-infrastructure/hyperion-guide",id:"wax-infrastructure/hyperion-guide",title:"Full/Partial History nodes using Hyperion",description:"Having stable accessibility to consumable data is essential for Web3 applications on a blockchain. There are many use-cases for history data like accounting, taxes, transaction tracking, portfolio management etc.",source:"@site/docs/wax-infrastructure/hyperion-guide.md",sourceDirName:"wax-infrastructure",slug:"/wax-infrastructure/hyperion-guide",permalink:"/waxdevelopers_test/docs/wax-infrastructure/hyperion-guide",draft:!1,editUrl:"https://github.com/3dkrender/waxdeveloper_test/tree/main/docs/wax-infrastructure/hyperion-guide.md",tags:[],version:"current",frontMatter:{title:"Full/Partial History nodes using Hyperion",nav_order:142,layout:"default",parent:"WAX Infrastructure Guides","lang-ref":"Full/Partial History nodes using Hyperion",lang:"en"},sidebar:"tutorialSidebar",previous:{title:"Atomic Assets/Market API",permalink:"/waxdevelopers_test/docs/wax-infrastructure/atomic-api-guide"},next:{title:"WAX Infrastructure Guides",permalink:"/waxdevelopers_test/docs/wax-infrastructure/"}},l={},p=[{value:"Pre-requisites/Requirements:",id:"pre-requisitesrequirements",level:3},{value:"Bare-Metal Infra providers:",id:"bare-metal-infra-providers",level:4},{value:"Setup and Installation:",id:"setup-and-installation",level:3},{value:"1. Update the default pacakages and install new ones",id:"1-update-the-default-pacakages-and-install-new-ones",level:5},{value:"2. For better CPU performance:",id:"2-for-better-cpu-performance",level:5},{value:"3. Create disk partitions",id:"3-create-disk-partitions",level:5},{value:"4. Increase the Swap size as its usually small on the servers from Hetzner and Leaseweb.",id:"4-increase-the-swap-size-as-its-usually-small-on-the-servers-from-hetzner-and-leaseweb",level:5},{value:"5. Create ZFS storage pool based on your requirements with zraid or mirror etc modes. A good resource to do calculations on disk sizes: http://www.raidz-calculator.com/",id:"5-create-zfs-storage-pool-based-on-your-requirements-with-zraid-or-mirror-etc-modes-a-good-resource-to-do-calculations-on-disk-sizes-httpwwwraidz-calculatorcom",level:5},{value:"6. Elasticsearch v7.17.X Setup &amp; Installation:",id:"6-elasticsearch-v717x-setup--installation",level:5},{value:"Replace the following sections in the ES config file",id:"replace-the-following-sections-in-the-es-config-file",level:6},{value:"Heap Size Configuration",id:"heap-size-configuration",level:6},{value:"Allow Memory Lock",id:"allow-memory-lock",level:6},{value:"Start Elasticsearch",id:"start-elasticsearch",level:6},{value:"Set Up Minimal Security",id:"set-up-minimal-security",level:6},{value:"7. Kibana Installation using Apt package:",id:"7-kibana-installation-using-apt-package",level:5},{value:"Configuration:",id:"configuration",level:6},{value:"Start Kibana",id:"start-kibana",level:6},{value:"8. Node JS installation:",id:"8-node-js-installation",level:5},{value:"9.Redis installation",id:"9redis-installation",level:5},{value:"Update Redis Supervision Method",id:"update-redis-supervision-method",level:6},{value:"Restart Redis",id:"restart-redis",level:6},{value:"10. Pm2 installation",id:"10-pm2-installation",level:5},{value:"11. RabbitMq Installation",id:"11-rabbitmq-installation",level:5},{value:"12. Setup &amp; Install Hyperion",id:"12-setup--install-hyperion",level:5},{value:"Setup:",id:"setup",level:6},{value:"Running Hyperion:",id:"running-hyperion",level:6}],d={toc:p},c="wrapper";function u(e){let{components:t,...a}=e;return(0,r.kt)(c,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Having stable accessibility to consumable data is essential for Web3 applications on a blockchain. There are many use-cases for history data like accounting, taxes, transaction tracking, portfolio management etc. "),(0,r.kt)("p",null,"There are multiple history solutions today that offers different features and functionalities, they all have different requirements in terms of infrastructure. The following guide presents the steps needed to setup a scalable and resilient Hyperion based history solution."),(0,r.kt)("h3",{id:"pre-requisitesrequirements"},"Pre-requisites/Requirements:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"API node Hardware(minimum specs):")," Multi-threaded CPU with at-least 4gHZ CPU speed or above, 64GB RAM, 14TB SSD(currenyly disk size is increasing around 25-30GB/day so plan accordingly) -  For ElasticSearch ","[Also it's recommended to have multi-node ES clusters for higher throughput]"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Full State-History node Hardware(recommended specs):")," i9 CPU, 128GB RAM, 6TB NVME SSD ","[For a partial state-history, you can have lower specs or have it on the same server as Hyperion. This can also be started from a snapshot]"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Hyperion version:")," v3.3.5 or above"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Dependencies:")," Elasticsearch 7.17.X, RabbitMQ, Redis, Node.js v16, PM2"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"OS:")," Ubuntu20.04 (recommended)")),(0,r.kt)("h4",{id:"bare-metal-infra-providers"},"Bare-Metal Infra providers:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.hetzner.com/dedicated-rootserver",title:"Hetzner"},"Hetzner")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://www.leaseweb.us/dedicated-servers",title:"Leaseweb"},"Leaseweb"))),(0,r.kt)("h3",{id:"setup-and-installation"},"Setup and Installation:"),(0,r.kt)("p",null,"After securing the servers and setting up the boot configuration and appropriate RAID modes, you can login to the server and follow the next commands below: "),(0,r.kt)("p",null,"[Recommendation - Only setup root partition in Raid1 or Raid5 modes for now. We shall partition the disks later on after the boot and allocate them to a ZFS pool]"),(0,r.kt)("h5",{id:"1-update-the-default-pacakages-and-install-new-ones"},"1. Update the default pacakages and install new ones"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"apt-get update && apt-get install -y vim htop aptitude git lxc-utils zfsutils-linux netfilter-persistent sysstat ntp gpg screen zstd\n")),(0,r.kt)("h5",{id:"2-for-better-cpu-performance"},"2. For better CPU performance:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"apt-get install -y cpufrequtils\necho 'GOVERNOR=\"performance\"' | tee /etc/default/cpufrequtils\nsystemctl disable ondemand\nsystemctl restart cpufrequtils\n")),(0,r.kt)("h5",{id:"3-create-disk-partitions"},"3. Create disk partitions"),(0,r.kt)("p",null,"First step is to determine the disks and their names using the commands below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"fdisk -l\n")),(0,r.kt)("p",null,"Now after identifying the disk names, let's partition them using the example command below, we need to create two partitions One for Swap and One for ZFS storage pool."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"cfdisk /dev/nvme0n1\n")),(0,r.kt)("p",null,"Do the above for all the disks on your server."),(0,r.kt)("h5",{id:"4-increase-the-swap-size-as-its-usually-small-on-the-servers-from-hetzner-and-leaseweb"},"4. Increase the Swap size as its usually small on the servers from Hetzner and Leaseweb."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"mkswap /dev/nvme0n1p5\nmkswap /dev/nvme1n1p5\n")),(0,r.kt)("p",null,"Now let's add the Swap pools to the System's FileSystem table by editing the file below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"cat >>/etc/fstab <<'EOT'\n/dev/nvme0n1p5     none            swap            defaults,pri=-2 0 0\n/dev/nvme1n1p5     none            swap            defaults,pri=-2 0 0\n/dev/nvme2n1p5     none            swap            defaults,pri=-2 0 0\nEOT\n")),(0,r.kt)("p",null,"After editing, let's enable the newly added Swap pool using the command below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"swapon -a\n")),(0,r.kt)("h5",{id:"5-create-zfs-storage-pool-based-on-your-requirements-with-zraid-or-mirror-etc-modes-a-good-resource-to-do-calculations-on-disk-sizes-httpwwwraidz-calculatorcom"},"5. Create ZFS storage pool based on your requirements with zraid or mirror etc modes. A good resource to do calculations on disk sizes: ",(0,r.kt)("a",{parentName:"h5",href:"http://www.raidz-calculator.com/"},"http://www.raidz-calculator.com/")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"zpool create -o ashift=12 zfast raidz /dev/nvme0n1p6 /dev/nvme1n1p6 /dev/nvme2n1p6 [--\x3e adopt the partition names accordingly]\nzfs set atime=off zfast\nzfs set compression=lz4 zfast [--\x3enot really needed as ES already compresses the data]\nzfs create -o mountpoint=/home zfast/home [--\x3eCreates mountpoint]\n")),(0,r.kt)("hr",null),(0,r.kt)("p",null,"Now that we have setup the server and disk storage in a good way, let's go ahead with the next steps to setup the Hyperion related dependencies."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://hyperion.docs.eosrio.io/manual_installation/"},"https://hyperion.docs.eosrio.io/manual_installation/")),(0,r.kt)("h5",{id:"6-elasticsearch-v717x-setup--installation"},"6. Elasticsearch v7.17.X Setup & Installation:"),(0,r.kt)("p",null,"The following steps are for a single node ES cluster but it is recommended to have a multi-node ES cluster for scalability & resiliency. Setup a minimum of 3 node ES cluster  so ES shards can be distributed and replicas are created. In addition use the Cross-Cluster replication in different data centres for geo resiliency."),(0,r.kt)("p",null,"For multi node ES cluster setup, refer:"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://www.elastic.co/guide/docs/elasticsearch/reference/7.17/scalability.html"},"https://www.elastic.co/guide/docs/elasticsearch/reference/7.17/scalability.html"),"\n",(0,r.kt)("a",{parentName:"p",href:"https://www.elastic.co/guide/docs/elasticsearch/reference/current/add-elasticsearch-nodes.html"},"https://www.elastic.co/guide/docs/elasticsearch/reference/current/add-elasticsearch-nodes.html")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"ES Installation using Apt package:")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\nsudo apt-get install apt-transport-https\necho "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list\nsudo apt-get update && sudo apt-get install elasticsearch\n')),(0,r.kt)("p",null,"Now, let's create new directories on the ZFS storage pool so that ES data & logs can be stored there instead of default directories:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"cd /home\nmkdir es-data\nmkdir es-logs\nchown -R elasticsearch:elasticsearch es-data/\nchown -R elasticsearch:elasticsearch es-logs/\n")),(0,r.kt)("p",null,"After creating the directories and fixing the folder permissions, let's edit the ES config by editing the file below:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"vim /etc/elasticsearch/elasticsearch.yml\n")),(0,r.kt)("h6",{id:"replace-the-following-sections-in-the-es-config-file"},"Replace the following sections in the ES config file"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"# ---------------------------------- Cluster -----------------------------------\ncluster.name: hyp-cluster\nbootstrap.memory_lock: true\n# ----------------------------------- Paths ------------------------------------\npath.data: /home/es-data\n# Path to log files:\npath.logs: /home/es-logs\n")),(0,r.kt)("h6",{id:"heap-size-configuration"},"Heap Size Configuration"),(0,r.kt)("p",null,"For a optimized heap size, check how much RAM can be allocated by the JVM on your system. Run the following command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"java -Xms16g -Xmx16g -XX:+UseCompressedOops -XX:+PrintFlagsFinal Oops | grep Oops\n")),(0,r.kt)("p",null,"Check if UseCompressedOops is true on the results and change -Xms and -Xmx to the desired value."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Note:")," Elasticsearch includes a bundled version of OpenJDK from the JDK maintainers. You can find it on /usr/share/elasticsearch/jdk"),(0,r.kt)("p",null,"After that, change the heap size by editting the following lines on"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"vim /etc/elasticsearch/jvm.options"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"-Xms25g\n-Xmx25g\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Note:")," Xms and Xmx must have the same value.\n",(0,r.kt)("strong",{parentName:"p"},"Warning:")," Avoid allocating more than 31GB when setting your heap size, even if you have enough RAM."),(0,r.kt)("h6",{id:"allow-memory-lock"},"Allow Memory Lock"),(0,r.kt)("p",null,"Override systemd configuration by running ",(0,r.kt)("inlineCode",{parentName:"p"},"sudo systemctl edit elasticsearch")," and add the following lines:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"[Service]\nLimitMEMLOCK=infinity\n")),(0,r.kt)("p",null,"Run the following command to reload units:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"sudo systemctl daemon-reload\n")),(0,r.kt)("h6",{id:"start-elasticsearch"},"Start Elasticsearch"),(0,r.kt)("p",null,"Start Elasticsearch and check the logs:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"sudo systemctl start elasticsearch.service\nsudo less /home/es-logs/hyp-cluster.log\n")),(0,r.kt)("p",null,"Enable it to run at startup:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"sudo systemctl enable elasticsearch.service\n")),(0,r.kt)("p",null,"And finally, test the REST API:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'curl -X GET "localhost:9200/?pretty" [Test if everything looks good]\n')),(0,r.kt)("h6",{id:"set-up-minimal-security"},"Set Up Minimal Security"),(0,r.kt)("p",null,"The Elasticsearch security features are disabled by default. To avoid security problems, we recommend enabling the security pack."),(0,r.kt)("p",null,"To do that, add the following line to the end of the file: ",(0,r.kt)("inlineCode",{parentName:"p"},"vim /etc/elasticsearch/elasticsearch.yml")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"xpack.security.enabled: true\n")),(0,r.kt)("p",null,"Restart Elasticsearch and set the passwords for the cluster:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"sudo systemctl restart elasticsearch.service\nsudo /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto\n")),(0,r.kt)("p",null,"Save the passwords somewhere safe, they'll be necessary for future purpose."),(0,r.kt)("p",null,"Now you can test the REST API using username and password:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'curl -X GET "http://localhost:9200/?pretty" -u elastic:<password>\n')),(0,r.kt)("h5",{id:"7-kibana-installation-using-apt-package"},"7. Kibana Installation using Apt package:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -\nsudo apt-get install apt-transport-https\necho "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list\nsudo apt-get update && sudo apt-get install kibana\n')),(0,r.kt)("h6",{id:"configuration"},"Configuration:"),(0,r.kt)("p",null,"Now let's edit the ",(0,r.kt)("inlineCode",{parentName:"p"},"vim /etc/kibana/kibana.yml")),(0,r.kt)("p",null,"Update the host address to 0.0.0.0 if needed for accessing it using the IP on the public network. By default it's set to localhost."),(0,r.kt)("p",null,"If you have enabled the security pack on Elasticsearch, you need to set up the password on Kibana:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'elasticsearch.username: "kibana_system"\nelasticsearch.password: "password"\n')),(0,r.kt)("h6",{id:"start-kibana"},"Start Kibana"),(0,r.kt)("p",null,"Start Kibana and check the logs:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"sudo systemctl start kibana.service\nsudo less /var/log/kibana/kibana.log\n")),(0,r.kt)("p",null,"Enable it to run at startup:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"sudo systemctl enable kibana.service\n")),(0,r.kt)("h5",{id:"8-node-js-installation"},"8. Node JS installation:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -\nsudo apt-get install -y nodejs\nnode  -v\n")),(0,r.kt)("h5",{id:"9redis-installation"},"9.Redis installation"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"sudo add-apt-repository ppa:redislabs/redis\nsudo apt-get -y update\nsudo apt-get -y install redis\nredis-server -v\n")),(0,r.kt)("h6",{id:"update-redis-supervision-method"},"Update Redis Supervision Method"),(0,r.kt)("p",null,"Change the ",(0,r.kt)("inlineCode",{parentName:"p"},"supervised")," configuration from ",(0,r.kt)("inlineCode",{parentName:"p"},"supervised no")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"supervised systemd")," on ",(0,r.kt)("inlineCode",{parentName:"p"},"/etc/redis/redis.conf")),(0,r.kt)("h6",{id:"restart-redis"},"Restart Redis"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"sudo systemctl restart redis-server\nsudo systemctl enable --now redis-server\nsudo systemctl status redis-server\nsudo systemctl status redis-server\nsudo systemctl unmask  redis-server.service\nsudo systemctl restart redis-server\nsudo systemctl status redis-server\n")),(0,r.kt)("h5",{id:"10-pm2-installation"},"10. Pm2 installation"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"npm install pm2@latest -g\npm2 startup\n")),(0,r.kt)("h5",{id:"11-rabbitmq-installation"},"11. RabbitMq Installation"),(0,r.kt)("p",null,"Copy the shell script from here and run it on the server: ",(0,r.kt)("a",{parentName:"p",href:"https://www.rabbitmq.com/install-debian.html#installation-methods"},"https://www.rabbitmq.com/install-debian.html#installation-methods")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"cd builds\nvim rabbit_install.sh\n")),(0,r.kt)("p",null,"After copying the script, you can now execute it:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"bash rabbit_install.sh\n")),(0,r.kt)("p",null,"Let's create directories in our ZFS Storage pool for RabbitMq:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"cd /home\nmkdir rabbitmq\nchown -R rabbitmq:rabbitmq rabbitmq/\n")),(0,r.kt)("p",null,"Add a env file in ",(0,r.kt)("inlineCode",{parentName:"p"},"/etc/rabbitmq")," so that we can charge the default directories:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"cd /etc/rabbitmq\nvim rabbitmq-env.conf\n")),(0,r.kt)("p",null,"Add the following lines to the config file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"RABBITMQ_MNESIA_BASE=/home/rabbitmq\nRABBITMQ_LOG_BASE=/home/rabbitmq/log\n")),(0,r.kt)("p",null,"Restart the rabbit server after updating the config:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"service rabbitmq-server restart\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'sudo rabbitmq-plugins enable rabbitmq_management\nsudo rabbitmqctl add_vhost hyperion\nsudo rabbitmqctl add_user hyper << password >>\nsudo rabbitmqctl set_user_tags hyper administrator\nsudo rabbitmqctl set_permissions -p hyperion hyper ".*" ".*" ".*"\nsudo rabbitmqctl add_vhost /hyperion\nsudo rabbitmqctl set_permissions -p /hyperion hyper ".*" ".*" ".*"\n')),(0,r.kt)("hr",null),(0,r.kt)("h5",{id:"12-setup--install-hyperion"},"12. Setup & Install Hyperion"),(0,r.kt)("p",null,"Now we have finished the dependencies setup, let's go ahead and start the actual Hyperion software installation."),(0,r.kt)("p",null,"We have two options now:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"To install and sync everything from scratch"),(0,r.kt)("li",{parentName:"ol"},"Use ES snapshots to sync the data and then start the Hyperion instance.")),(0,r.kt)("p",null,"Note: If you are using ES snapshots from a snapshot service provider, go to Kibana dev mode and enter the following commands:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'PUT _snapshot/eosphere-repo\n{\n   "type": "url",\n   "settings": {\n       "url": "https://store1.eosphere.io/wax/hyperion/snapshot/"\n   }\n}\n\n\nPOST _snapshot/eosphere-repo/wax_snapshot_2022.02.01/_restore\n{\n  "indices": "*,-.*"\n}\n')),(0,r.kt)("h6",{id:"setup"},"Setup:"),(0,r.kt)("p",null,"Clone the latest codebase and install the hyperion:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"git clone https://github.com/eosrio/hyperion-history-api.git\ncd hyperion-history-api\nnpm install\n")),(0,r.kt)("p",null,"Now it's installed, we have to setup the connections and the chain configuration."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Follow the guide ",(0,r.kt)("a",{parentName:"li",href:"https://hyperion.docs.eosrio.io/connections/"},"here"),' "here") to setup connections.json file. or find the example below:')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'{\n  "amqp": {\n    "host": "127.0.0.1:5672",\n    "api": "127.0.0.1:15672",\n    "protocol": "http",\n    "user": "hyper",\n    "pass": "<Enter your RMQ password>",\n    "vhost": "hyperion",\n    "frameMax": "0x10000"\n  },\n  "elasticsearch": {\n    "protocol": "http",\n    "host": "127.0.0.1:9200",\n    "ingest_nodes": [\n      "127.0.0.1:9200"\n    ],\n    "user": "elastic",\n    "pass": "<Enter the elastic user password from step 6>"\n  },\n  "redis": {\n    "host": "127.0.0.1",\n    "port": "6379"\n  },\n  "chains": {\n    "wax": {\n      "name": "Wax",\n      "ship": "ws://<Enter your Ship node endpoint here>:8080",\n      "http": "http://<Enter your API node endpoint here>:8888",\n      "chain_id": "1064487b3cd1a897ce03ae5b6a865651747e2e152090f99c1d19d44e01aea5a4",\n      "WS_ROUTER_HOST": "127.0.0.1",\n      "WS_ROUTER_PORT": 7001\n    }\n  }\n}\n')),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Follow the guide ","[here]",(0,r.kt)("a",{parentName:"li",href:"https://hyperion.docs.eosrio.io/chain/"},"https://hyperion.docs.eosrio.io/chain/"),') "here") to setup wax.config.json file')),(0,r.kt)("h6",{id:"running-hyperion"},"Running Hyperion:"),(0,r.kt)("p",null,"There are two parts to Hyperion, one is Indexer and the other is the API."),(0,r.kt)("p",null,'When you start with Indexer the first step is to run it with the ABI scan mode. And once the ABI scan is done you can start it back without it. The Hyperion Indexer is configured to perform an abi scan ("abi_scan_mode": true) as default. '),(0,r.kt)("p",null,"You can use the following commands to run and stop the indexer."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"./start.sh wax-indexer\n./stop.sh wax-indexer\n")),(0,r.kt)("p",null,"Once the indexer is synced, you can start it with the live mode and then start the API."),(0,r.kt)("p",null,"To start the API, you can use the following commands:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"./start.sh wax-api\n./stop.sh wax-api\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Note:")," If you have any further questions about how to use Hyperion, please write them here: ",(0,r.kt)("a",{parentName:"p",href:"https://t.me/EOSHyperion"},"https://t.me/EOSHyperion")),(0,r.kt)("hr",null),(0,r.kt)("p",null,"For setting up the partial history guide: ",(0,r.kt)("a",{parentName:"p",href:"https://medium.com/waxgalaxy/lightweight-wax-hyperion-api-node-setup-guide-f080a7d4a5b5"},"https://medium.com/waxgalaxy/lightweight-wax-hyperion-api-node-setup-guide-f080a7d4a5b5")))}u.isMDXComponent=!0}}]);